#!/usr/bin/env python

# import urllib2
import sys
import os
import re

from urllib2 import urlopen
from HTMLParser import HTMLParser


class ImgUrParser(HTMLParser):

    def __init__(self):
        # disclaimer: HTMLParser is old style class, do not support 'super'
        # initialization style :/
        HTMLParser.__init__(self)

        self._images = []

    def handle_starttag(self, tag, attrs):
        if tag == "a":
            for attr, value in attrs:
                # XXX regex to get jpg files
                if attr == "href" and value.endswith(".jpg"):

                    self._images.append("https:%s" % value)

    def images(self):
        return self._images


class Downloader():

    def __init__(self, path, name):
        self._path = path
        self._name = name

        self._download_path =  os.path.join(path, dir_name)

    def download(self, images=[]):
        if not images:
            return

        num_images = len(images)

        if not os.path.exists(self._download_path):
            print "Creating %s/ on %s" % (self._name, self._path)
            os.mkdir(self._download_path)

        print "Downloading images to %s/" % self._download_path

        for index, image in enumerate(images):
            image_file = image.split('/')[-1]

            print "    (%s/%s) %s..." % (index + 1, num_images, image_file)

            image_file = os.path.join(self._download_path, image_file)
            if os.path.isfile(image_file):
                print "        File already exists, skipping..."
                continue

            try:
                f = open(image_file, 'wb')
                f.write(urlopen(image).read())
                f.close()
            except Exception as e:
                print e


if __name__ == '__main__':
    # TODO
    url = "https://imgur.com/gallery/SwhjO"
    # TODO accept a single image as well?

    # TODO exceptions, move to parser
    # -- parser
    print "Parsing %s" % url

    f = urlopen(url)
    html = f.read()
    f.close()

    parser = ImgUrParser()
    html = parser.unescape(html)
    parser.feed(html)

    num_images = len(parser.images())
    print "    Found %s images" % num_images
    # -- parser

    # -- downloader
    download_path = os.getcwd() # XXX set a default one
    dir_name = url.split('/')[-1]

    downloader = Downloader(download_path, dir_name)
    downloader.download(parser.images())
    # -- downloader
